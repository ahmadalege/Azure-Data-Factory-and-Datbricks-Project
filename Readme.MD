![](Diagrams/Architecture/architecture.png)

# ğŸš€ Azure End-to-End Data Engineering Project

## Medallion Architecture with ADF, Databricks & ADLS Gen2

### ğŸ“Œ Project Overview
This project implements a full end-to-end Azure data engineering platform using modern best practices and the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**.

The solution ingests data from:
- ğŸŒ Rebrickable API (cloud REST API)
- ğŸ—„ï¸ Local PostgreSQL (DVD Rentals â€“ Pagila dataset) via Self-Hosted Integration Runtime

The platform processes data using:
- Azure Data Factory for orchestration and ingestion
- Azure Data Factory Mapping Data Flows for Silver-layer transformations
- Azure Databricks + Delta Lake for advanced processing and Gold-layer modeling
- Azure Data Lake Storage Gen2 as the central data lake
- Azure Key Vault for secrets management
- GitHub for version control and collaboration

The final output exposes analytics-ready Gold tables suitable for BI, reporting, and downstream consumption.

### ğŸ—ï¸ Architecture
High-level architecture:
1. Source systems (API + PostgreSQL)
2. ADF ingestion pipelines
3. ADLS Gen2 (Bronze, Silver, Gold containers)
4. Databricks transformation layer
5. Delta Lake tables
6. GitHub-integrated CI-style workflow


### ğŸ§± Medallion Architecture Design

#### ğŸ¥‰ Bronze â€“ Raw Ingestion
**Purpose:**  
Store data exactly as received, without transformation.

**Sources:**
- Rebrickable REST API (JSON)
- PostgreSQL Pagila DVD rental database

**Characteristics:**
- Append-only
- Source-aligned
- No business logic

---

#### ğŸ¥ˆ Silver â€“ Clean & Conformed
**Purpose:**  
Standardize, clean, deduplicate, and type data.

**Implemented using:**
- ADF Mapping Data Flows (DVD Rentals)
- Databricks notebooks (Rebrickable)

**Transformations include:**
- Data type enforcement
- Column renaming and standardization
- Null handling
- Deduplication using window functions
- Derived business attributes

---

#### ğŸ¥‡ Gold â€“ Business & Analytics Layer
**Purpose:**  
Provide business-ready, modeled, and aggregated datasets.  
Implemented fully in Databricks using Delta Lake.

**Rebrickable Gold Tables:**
- `gold.dim_minifigs`  
  Curated LEGO minifigure dimension.  
  Includes:
  - Surrogate key
  - Business attributes
  - Audit timestamps
  - Derived analytics columns:
    - superhero flag
    - name length
    - modified year

- `gold.fact_minifigs_stats`  
  Snapshot analytics fact table.  
  Includes:
  - snapshot date
  - total minifigs
  - average parts
  - min / max parts
  - superhero count

**DVD Rentals Gold Tables:**  
Star-schema style model:
- `gold.dim_customer`
- `gold.dim_film`
- `gold.fact_rentals`

Built from cleaned silver tables using joins, derivations, and filtering.


---

### âš™ï¸ Technology Stack
- Azure Data Factory
- Azure Databricks
- Azure Data Lake Storage Gen2
- Delta Lake
- Azure Key Vault
- Azure Entra ID (Service Principal)
- GitHub integration
- Self-Hosted Integration Runtime (on-prem PostgreSQL)

---

### ğŸ” Security & Best Practices
- Secrets stored in Azure Key Vault
- Databricks authentication using Service Principal OAuth
- No secrets committed to GitHub
- ADLS access controlled via IAM RBAC
- SHIR used for secure on-prem connectivity

---

### ğŸ” Data Engineering Workflow
1. Ingest API and PostgreSQL data with ADF
2. Store raw data in Bronze container
3. Clean and standardize into Silver
4. Transform and model in Databricks
5. Write curated Delta tables to Gold
6. Version all assets in GitHub

---

### ğŸ—‚ï¸ Repository Structure

```text
azure-data-engineering-project/
â”œâ”€â”€ Azure Data Factory/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ dataflows/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ linkedServices/
â”‚   â””â”€â”€ triggers/
â”œâ”€â”€ databricks/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ screenshots/
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ resource-list.md
â””â”€â”€ README.md
```


---

### ğŸ§ª Key Engineering Features Demonstrated
- REST API ingestion
- On-prem PostgreSQL ingestion using SHIR
- Medallion architecture
- Mapping Data Flows
- Delta Lake
- Slowly evolving datasets
- Star schema modeling
- Snapshot fact tables
- Secure secret handling
- Git-integrated ADF
- Databricks (Pyspark)


---

### â–¶ï¸ How to Run the Project
1. Deploy Azure resources
2. Configure Key Vault secrets
3. Connect ADF to GitHub
4. Configure SHIR
5. Trigger Bronze pipelines
6. Run Silver transformations in ADF
7. Execute Databricks Gold notebooks
8. Validate Gold Delta tables

---

### ğŸ¯ Learning Outcomes
This project demonstrates the ability to:
- Design cloud data platforms
- Implement medallion architecture
- Build production-style ingestion pipelines
- Apply data modeling concepts
- Secure enterprise data systems
- Version control data engineering assets
- Work with both cloud and on-prem data
- Databricks mastery

---

### ğŸ‘¤ Author
**[Ahmad Olaitan Alege]**  
 2026


